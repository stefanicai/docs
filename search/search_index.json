{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository contains notes I take for my own use when I learn something new. They are not designed to be used by anyone else, but if they're helpful ... why not \ud83e\udd37\u200d\u2642\ufe0f</p>"},{"location":"datascience/data-processing/","title":"Data processing","text":"<p>Data processing involves several steps.</p>"},{"location":"datascience/data-processing/#replace-null-values","title":"Replace null values","text":"<p>Places where we don't have values for relevant attributes. We can replace them with <code>mean</code> or <code>meadian</code> for numeric. If it's not a number, we could use <code>mode</code>.</p>"},{"location":"datascience/data-processing/#removing-duplicates","title":"Removing duplicates","text":"<p>If we have the same data several times in our data set</p>"},{"location":"datascience/data-processing/#clean-up-the-data","title":"Clean-up the data","text":"<p>Numbers are not correct or not in the right formats etc. Unusable for whatever reason</p>"},{"location":"datascience/data-processing/#convert-text-values-to-numbers","title":"Convert text values to numbers","text":"<p>This can be cases where numeric data is stored as text in the source data set - e.g. <code>\"15\" -&gt; 15</code></p> <p>Or it can be representing text data in a numeric way. E.g. say we have the gender and we decide to use something like this <pre><code>female = 0\nmale = 1\nother = 2\n</code></pre></p>"},{"location":"datascience/decisions/","title":"Paradigms in Data Science","text":"<p>Both the below are critical. In short, inferential is about the methodology to draw conclusions based on data, computational is supporting that as well as sometimes the representation of the data in a nicer format.</p>"},{"location":"datascience/decisions/#inferential-data-science","title":"Inferential data science","text":"<p>Inference means: I'm seeing something/numbers, what does it imply/mean? It is about answering a question.</p> <p>Inferential data science focuses on drawing conclusions and making predictions from data, using statistical models and hypothesis testing</p> <ul> <li>Making predictions based on sample data.</li> <li>use statistical methods to draw conclusions/inferences from data</li> <li></li> </ul>"},{"location":"datascience/decisions/#computational-data-science","title":"Computational data science","text":"<p>This is about any kind of calculation - computer based or not. It is about exploring the data.</p> <p>Leverages algorithms, cost and complexity of training large models etc.</p> <p>Computational can also be used for generating insights. Explaining in a nicer way (e.g. graphs, charts etc) of conclusions drawn from inferential</p>"},{"location":"datascience/python/libraries/numpy/","title":"Numpy","text":"<p>Numpy is a library that enables better handling of arrays and matrixes (arrays of arrays). </p> <p>Standard library arrays have some limitations. They have to contain the same type of value. More importantly, they don't perform that well for large arrays.</p> <pre><code>import numpy as np\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#arrays","title":"Arrays","text":"<pre><code>import numpy as np\n\n# create standard array\narr_str = [ \"value1\", \"value2\" ]\n\n# convert \nnp_arr_str = np.array(arr_str)\n\nprint(type(np_arr_str))\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#nparrange","title":"np.arrange","text":"<p>Creates a numpy array with values between two numbers (first included, last not), with a specific step.</p> <p>These statements are equivalent:</p> <pre><code>np.array(list(range(0, 10))) # uses standard library range\n\nnp.arrange(start=0, stop=10, step=1) # using keyword params\n\nnp.arrange(0, 10) # using positional params\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#nplinspace-npzeros-npones-npeye","title":"np.linspace, np.zeros, np.ones, np.eye","text":"<p>Used to generate arrays.</p> <p><code>np.linspace</code> creates an evenly distributed array, starting from a start value x1, ending with and end value x2, containing a specific number of elements y. It is used like <code>np.linspace(x1, x2, y)</code></p> <p><code>np.zeros</code> - creates a matrix of zero values. The size of the matrix is given by the first parameter, an array of two elements, first being the row number, second the column. You can also specify the type of each element, float being the default. E.g. <code>np.zeros( [rows, columns], float)</code></p> <p><code>np.ones</code> - is identical to the above, but creates matrixes of ones.</p> <p><code>np.eye</code> - returns a square matrix, with <code>1</code> on the first diagonal (left top corner to right bottom corner), and zeros otherwise. It's used like <code>np.eye(n, float)</code></p>"},{"location":"datascience/python/libraries/numpy/#npreshape","title":"np.reshape","text":"<p>Reorganizes an array into a matrix</p> <pre><code># create an array 1-10\narr = np.arrange(0,10)\n\n# convert it to a matrix\narr_matrix = arr.reshape(2, 5)\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#arithmetic-operations-on-arrays-and-matrixes","title":"Arithmetic operations on arrays and matrixes","text":"<p>Numpy arrays enable us to add, remove values of arrays, rather than concatenate them.</p> <pre><code># create two arrays\narr7 = [1 2 3 4 5]\narr8 = [3 4 5 6 7]\n\nprint('Addition: ',arr7+arr8)\nprint('Subtraction: ',arr8-arr7)\nprint('Multiplication:' , arr7*arr8)\nprint('Division:', arr7/arr8)\nprint('Inverse:', 1/arr7)\nprint('Powers:', arr7**arr8) # in python, powers are achieved using **\n\n# the above will result in an array of the same dimension (5 in our case) where each element is the result of applying the specific operation between the corresponding values on the same position\n# Addition:         [4 6 8 10 12]\n# Subtraction:      [2 2 2 2 2]\n# Multiplication:   [3 8 15 24 35]\n# etc\n</code></pre> <p>The same goes on <code>matrixes</code>, the operations will apply between elements at the same position.</p> <p>Note: If the operation is undefined, e.g. dividing by 0, the result will be <code>inf</code> and a warning will be printed</p>"},{"location":"datascience/python/libraries/numpy/#transpose-matrix-nptranpose-or-via-t-property","title":"Transpose matrix - np.tranpose or via .T property","text":"<pre><code># create a matrix\nmatr = np.arrange(1,10).reshape(3,3)\n\n# get the transpose function\ntransp_matr = np.transpose(matr)\n\n# get the transpose via the T property\ntransp_matr = matr.T\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#npmin-npmax-npmean-npstd","title":"np.min, np.max, np.mean, np.std","text":"<p>They are utility functions to apply on a matrix or array.</p> <pre><code># if matr is a matrix and arr is an array\nnp.min(matr)\nnp.min(arr)\n\n# calculate the mean\nnp.mean(arr)\n\n# calculate standard distribution\nnp.std(arr)\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#trigonometric-functions","title":"Trigonometric functions","text":""},{"location":"datascience/python/libraries/numpy/#npsin-npcos-nptan-npexp-nplog","title":"np.sin, np.cos, np.tan, np.exp, np.log","text":"<p>Allows applying trigonometric and logarithmic functions easily.</p> <pre><code>np.sin(4)\n\nnp.log10(8) # log base 10\n\nnp.log(2) # uses 'e' number as default base\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#generate-random-numbers","title":"Generate random numbers","text":""},{"location":"datascience/python/libraries/numpy/#nprandomrand","title":"np.random.rand","text":"<p>Generates an array or matrix of random values from the uniform distribution over [0, 1]</p> <pre><code># array\nrand_arr = np.random.rand(5)\n\n# matrix\nrand_mat = np.random.rand(2, 3)\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#nprandomrandn","title":"np.random.randn","text":"<p>The same as above, but generates numbers from a standard distribution (mean ~= 0, std ~= 1)</p>"},{"location":"datascience/python/libraries/numpy/#nprandomrandint","title":"np.random.randint","text":"<p>Same as above, but the values are integers</p> <pre><code># array of 10, values from 1 to 5\nnp.random.randint(1, 5, 10)\n\n# matrix or 5x5, values from 1 to 10\nnp.random.randint(1, 10, [5, 5])\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#accessing-elements","title":"Accessing elements","text":"<p>Accessing elements can be done via the usual square brackets <code>arr[index]</code>, but it can also be done to access several elements at the same time</p> <pre><code>rand_arr = np.random.randn(10)\n\n# access one element - third element\nrand_arr[2]\n\n# access all elements bigger than 1, via condition\nrand_arr[rand_arr &gt; 1]\n\n# access elements between indexes 2 inclusive, and 4 exclusive\nrand_arr[2:4]\n</code></pre> <p>For matrixes, it would be the same. The only difference is that matrixes allow an extra format compared to the standard library. All the other operations otherwise apply exactly the same for matrixes as for arrays.</p> <pre><code>rand_mat = np.random.randn(10, 10)\n# these are equivalent\nrand_mat[2][5] == rand_mat[2, 5]\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#changing-array-and-matrixes-values","title":"Changing array and matrixes values","text":"<p>Other than the usual, these are allowed too</p> <pre><code># change values of elements 3 and 4 to be equal to the value 1\nrand_arr[3:5] = 1\n# a different way to get the same outcome as above\nrand_arr[3:5] = [1, 1]\n\n# change all elements that have a value more than 1, to be 65 - using logical references\nrand_arr[rand_arr &gt; 1] = 65\n</code></pre> <p>For matrixes <pre><code># change all elements of a matrix to the value 3\nrand_mat[:] = 3\n\n# change the elements of the below submatrix to 3\nrand_mat[2:5,1:3] = 3\n</code></pre></p>"},{"location":"datascience/python/libraries/numpy/#use-copy-to-keep-original-arraymatrix-intact","title":"Use copy to keep original array/matrix intact","text":"<p>When selecting elements from an array or matrix, the result is a pointer to the original array/matrix. If we don't want to modify the original array/matrix, we must use the <code>copy</code> function.</p> <pre><code>rand_mat = np.random.randint(1, 10, [5, 5])\n\n# selects the first column and creates a copy to detatch it from the original rand_mat matrix\nsub_mat = rand_mat[0:5, 0:1].copy()\n\n# sets the value of the selection to 1. Because we used the copy function above, this will only change sub_mat. Otherwise it would have changed rand_mat too\nsub_mat[:] = 1\n</code></pre>"},{"location":"datascience/python/libraries/numpy/#save-to-disk-and-load","title":"Save to disk and load","text":"<pre><code># one array\nnp.save('path/to/file', arr)\nloaded_arr = np.load('path/to/file.npy') # note the extension required here, not provided at save\nprint(loaded_arr) #just one so print it\n\n# several arrays/matrixes\nnp.savez('path/to/files', matr_name1=matr1, matr_name2=matr2)\nloaded_multi = loaded_arr = np.load('path/to/file.npz')\nprint(loaded_multi['matr_name1']) # since there's more than one, we can only access them like this\n</code></pre> <p>Or as text (e.g. csv) <pre><code>np.savetxt('path/to/file.txt', matr1, delimiter=',') # note file extension provided here, but not in previous examples\nmatr1 = np.loadtxt('path/to/file.txt', delimiter=',')\n</code></pre></p>"},{"location":"datascience/python/libraries/pandas/","title":"Pandas","text":"<p>Pandas is a library used to work with series or matrixes of values.</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#series","title":"Series","text":"<p>One dimensional labeled array, able to hold any data. The labels are called <code>index</code> <pre><code># standard array, but it could also be a numpy array\narr = [ 12, 3, 4 ]\n\n# create a series using default numbered indexes\npd.Series(arr)\n\n# use custom indexes\nmed_series = pd.Series(arr, index = [ 'ibuprofen', 'paracetamol', 'nurofen' ])\n</code></pre></p>"},{"location":"datascience/python/libraries/pandas/#mathematical-operations-on-series","title":"Mathematical operations on series","text":"<pre><code># med prices\nmed_prices = pd.Series([ 12, 3, 4 ], index = [ 'ibuprofen', 'paracetamol', 'nurofen' ])\n\n# add 5 to each element of the series\nnew_med_prices = med_prices + 5\n\n# we can check the price diffence\ndiff_prices = new_med_prices - med_prices\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#accessing-series","title":"Accessing Series","text":"<pre><code># by index - all apply like for arrays\nmed_prices[0]\n# first 3 elements, so index 0-2\nmed_prices[:3]\n# last 2 elements\nmed_prices[-2:]\n# multiple elements\nmed_prices[0,2]\n\n# by index label\nmed_prices['nurofen']\n# multiple - use array in array\nmed_prices[ [ 'nurofen', 'ibuprofen' ] ]\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#dataframe","title":"DataFrame","text":"<p>Two dimensional tabular data structure with labeled axes (rows and columns)</p> <pre><code>students = [ 'Adi', 'Juli', 'Lux', 'Greg' ]\n\n# from array (or it could be Series)\nstud_df = pd.DataFrame(students, columns=['Student'])\n\ngrades = [ 5, 7, 4, 9 ]\n# table from dictionary/map\nstud_grades = pd.DataFrame({'Student':students, 'Grade':grades})\n</code></pre> <p>Above creates the table: || Student| Grade| |---|---|---| |0| Adi  |5| |1| Juli |7| |2| Lux  |4| |3| Greg |9|</p>"},{"location":"datascience/python/libraries/pandas/#accessing-dataframe","title":"Accessing DataFrame","text":"<pre><code># first two rows - row 0 and 1\nstud_grades[:2]\n# every 2 rows - use :: for that\nstud_grades[::2]\n# every 2 rows in reverse\nstud_grades[::-2]\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#using-loc-and-iloc","title":"Using <code>loc</code> and <code>iloc</code>","text":"<p>Both <code>loc</code> and <code>iloc</code> use the following format. The only difference is that <code>iloc</code> only uses indexes, while loc uses index labels</p> <p>dataframe.loc[row selection, column selection]</p> <pre><code># second row, index here is numeric\nstud_grades.loc[1]\n\n# get students grade for second and third student\nstud_grades.loc[ [1, 2], ['Grade'] ]\n# same via iloc\nstud_grades.iloc[ [1, 2], [1] ]\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#condition-based","title":"Condition based","text":"<pre><code># get students with passing grades\nstud_grades.loc[ stud_grades['Grade'] &gt;= 5 ]\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#utility-functions-head-tail-shape-info-min-max-unique-value_counts-mean-median-mode","title":"Utility functions:  head, tail, shape, info, min, max, unique, value_counts, mean, median, mode","text":"<p>Utility functions used to access various info <pre><code>stud_grades.min() or stud_grades['grade'].min()\n\n#info about the columns and types\nstud_grades.info()\n\n# top 5 rows\nstud_grades.head()\n\n# no of rows and columns. Note it's a property, not function\nstud_grades.shape\n\n# first mode in a multi mode data set. Not our case. Drop the index at the end if it's just one\nstud_grades.mode()[0]\n</code></pre></p>"},{"location":"datascience/python/libraries/pandas/#date-time-functions","title":"Date time functions","text":"<pre><code># when column is date\ndf1['date'].dt.strftime('%m/%d/%Y')\n# access month\ndf1['date'].dt.month\n\n# when field is not\npd.to_datetime(df['date'], dayFirst=True)\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#group-by","title":"Group By","text":"<p>Used to split data into groups <pre><code># median of the grades of students in the same location\nstud_grades.groupby(['location'])['grade'].median()\n</code></pre></p>"},{"location":"datascience/python/libraries/pandas/#addremove-column","title":"Add/Remove column","text":"<pre><code># add column\nstud_grades['location'] = ['bucharest', 'london', 'sydney', 'melbourne']\n\n# remove the new column in place. axis = 1 means column, while 0 is row\nstud_grades.drop('location', axis=1, inplace=True)\n\n# remove row with index 1 (second row). Note axis=0\nstud_grades.drop(1, axis=0)\n\n# reset index, since it's inconsistent after removal of row 1. drop=True removes the old index, otherwise it gets saved as a column labeled 'index'\nstud_grades.reset_index(drop=True, inplace=True)\n</code></pre> <p>Note: if you don't want to use <code>inplace</code> above to change the original data frame, use <code>copy</code> function to create a copy of the result, same as with numpy arrays</p>"},{"location":"datascience/python/libraries/pandas/#combining-dataframes","title":"Combining DataFrames","text":"<pre><code># df1 comes on top, df2 bottom, contact vertically. Would be horizontally if axis=1. Note, if columns are different, NaN will be used for missing values\npd.concat([df1, df2], axis=0)\n\n# use merge to merge two frames - same as a SQL outer join. Also possible left, right and inner joins/merge\npd.merge(df1, df2, how='outer', on='columnName')\n\n# join uses index labels, so no 'on'\ndf1.join(df2, how='left')\n</code></pre>"},{"location":"datascience/python/libraries/pandas/#applying-functions-to-dataframe","title":"Applying functions to DataFrame","text":"<pre><code># define function to calculate if student has passed\npassed = lambda grade: grade &gt;= 5\n\n# add column for status if a student passed the class\nstud_grades['passed'] = stud_grades['grade'].apply(passed)\n</code></pre>"},{"location":"datascience/python/libraries/scipy_stats/","title":"scipy.stats","text":"<p>Library used to calculate probabilities</p> <pre><code>import scipy.stats as stats\n</code></pre>"},{"location":"datascience/python/libraries/scipy_stats/#binomial","title":"Binomial","text":"<pre><code>from scipy.stats import binomial\n\n# probability mass function\nbinomial.pmf()\n\n# corelated distribution function. Pass a value, returns probability to happen\nbinomial.cdf()\n\n# percentile point given probability - oposite of cdf, pass a probability, returns value having that probability to happen.\nbinomial.ppf()\n</code></pre>"},{"location":"datascience/python/libraries/scipy_stats/#uniform","title":"Uniform","text":"<pre><code>from scipy.stats import uniform\n\n# probability mass function - for discrete\nuniform.pmf()\n\n# probability density function - for continuous\nuniform.pdf(x, loc=1, scale=4) # starts at 1 and has 4 steps, thus 1-5. x is the data\n\n# corelated distribution function. Pass a value, returns probability to happen\nbinomial.cdf(x=2, loc=1, scale=4) # probability that x is 2, for a uniform distribution from 1-5.\n\n# percentile point given probability - oposite of cdf, pass a probability, returns value having that probability to happen.\nbinomial.ppf(q=0.2, loc=1, scale=4) # value that has the probability of 20%, for a uniform distr from 1-5\n</code></pre>"},{"location":"datascience/python/libraries/visualisation/","title":"Visualisation","text":"Type X Variable Y Variable Purpose of analysis Type of chart How it looks Univariate Continuous - How the values on the X variable are distributed? Histogram, Distribution plot Histogram Distribution plot for car horsepower. Univariate Categorical - What is the count of observations in each category of X variable? Count plot Count plot for degree of students  Bivariate Continuous Continuous How Y is correlated to X? Scatter plot Scatter plot restaurant bill vs tip  Bivariate Time related (months, hours etc) Continuous How Y changes over time? Line plot Line plot of sales over a 10 days period  Bivariate Continuous Categorical How range of X varies for various category levels Box plot, Swarm plot Box plot of tip by meal time   Swarm plot tips per week day  Bivariate Categorical Categorical What is the number of % of records of X which falls under each category of Y Stacked bar plot Smokers/non-smokers vs fitness level  Type Variables Purpose of analysis Type of chart How it looks Multivariate Continuous (more than 2) How to visualize relationshipo across multiple combination of variables? Pair plot Multivariate Continuous(more than 2) How to visualize the spread of values in the data with color-encoding? Heatmap"},{"location":"datascience/python/libraries/visualisation/#libraries-matplotlib-seaborn","title":"Libraries - matplotlib &amp; seaborn","text":"<p><code>matplotlib</code> is for basic charts, <code>seaborn</code> is able to handle more complex charts and it builds on top of <code>matplotlib</code>, so it requires that to be imported too.</p> <p>Some important <code>matplotlib</code> functions: <code>plot()</code>, <code>hist()</code>, <code>bar()</code>, <code>pie()</code>, <code>scatter()</code>, <code>text()</code>, <code>legend()</code>, etc.</p> <p>Some important <code>seaborn</code> functions: <code>displot()</code>, <code>boxplot()</code>, <code>stripplot()</code>, <code>pairplot()</code></p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# if desired, also tell python to display the charts inline \n%matplotlib inline\n</code></pre>"},{"location":"datascience/python/libraries/visualisation/#histogram","title":"Histogram","text":"<p>A histogram is a univariate plot which helps us understand  the distribution of a continuous numerical variable. It breaks the range of continuous variables into an interval of equal length and then counts the number of observations in each interval.</p> <p><pre><code># load some data, assume it's cars info and has a price column\n#df = pd.read_csv('/path/to/file')\n\n# if we want to customize, we'll use 'matplotlib' properties\nplt.title('Histogram:Price')\nplt.xlim(1000, 3000) # limits between 1000 and 3000 for the x axis\nplt.xlabel('Price')\nplt.ylabel('Count')\n\n# histoplot for the 'price' column. It uses above settings and uses blue to drow the bars. It will group all data in 5 groups/bins/bars and each bin will be 200px wide. It will show the kernel density estimation curve\nsns.histplot(data=df, x='price', color='blue', bins=5, binwidth=200, kde=True)\n</code></pre> It will look something like: </p>"},{"location":"datascience/python/libraries/visualisation/#subplots","title":"Subplots","text":"<p>When you want to split the data and show several histograms, you could use subplots</p> <p><pre><code>g = sns.FacetGrid(df, col=\"body_style\")\ng.map(sns.histoplot, \"price\")\n</code></pre> It would look like this:</p> <p></p>"},{"location":"datascience/python/libraries/visualisation/#boxplot","title":"Boxplot","text":"<p>Shows the distribution of numerical data and skewness through displaying the data quartiles</p> <p><pre><code>sns.boxplot(data=df, x='curb_weight')\n</code></pre> Looks like this:</p> <p></p> <p><pre><code># or on both axis\nsns.boxplot(data=df, x='body_style', y='price')\n</code></pre> It will look like:</p> <p></p>"},{"location":"datascience/python/libraries/visualisation/#bar-graph","title":"Bar Graph","text":"<p>Used to show the counts of observations in each bin (or level or group) of categorical variable using bars</p>"},{"location":"datascience/statistics/concepts/","title":"Statistics concepts and terms","text":"<p>Say we have this set: [ 23, 29, 20, 32, 23, 21, 33, 25 ]</p>"},{"location":"datascience/statistics/concepts/#mean","title":"Mean","text":"<p>Mean is what we'd generally call the arithmetic average. Add all numbers and divide by number of values:</p> <pre><code>mean = sum([ 23, 29, 20, 32, 23, 21, 33, 25 ])/8 -&gt; 25.75\n</code></pre> <p>NOTE: Mean is sensitive to extremes, especially in a small sample.</p>"},{"location":"datascience/statistics/concepts/#median","title":"Median","text":"<p>The median is the number at the middle of the sorted data set. In our case if we order the data set, we get [ 20, 21, 23, 23, 25, 29, 32, 33 ].</p> <p>Since it's an even number of values, we have two medians, in our case 23 and 25. We calculate the median in this case as the average of those two values.</p> <pre><code>median = (23 + 25)/2 -&gt; 24\n</code></pre> <p>Note: Median isn't sensitive to outliers.</p>"},{"location":"datascience/statistics/concepts/#mode","title":"Mode","text":"<p>The number that shows the most in the data set. In our case, 23 is our mode, as we have it twice in the data set, while all other values are only once.</p> <p>If data has no repeating value or no value appears more than others, there's no <code>mode</code>. Note that includes the case when the values apear more than once, but all apear the same number of times.</p> <p>If we have several values that have the same number of occurences, we have a <code>bimodal or multiple modes</code> data set. We don't average them as we do for multiple medians.</p>"},{"location":"datascience/statistics/concepts/#range","title":"Range","text":"<p>The range is given by the biggest number minus smallest. For our data above, we have: <pre><code>range = 33 - 20 -&gt; 13\n</code></pre></p>"},{"location":"datascience/statistics/concepts/#standard-deviation","title":"Standard deviation","text":"<p>The average difference between each data sample and the mean. Formula is:</p> <pre><code>Standard deviation = radical (1/n * sum(xi - mean))^2)\n</code></pre>"},{"location":"datascience/statistics/concepts/#outliers","title":"Outliers","text":"<p>In many, though not all, cases, to be able to extract relevant information applicable to most of your data, you need to remove the outliers before processing it.</p> <p><code>outlier</code> = data that is significantly far from the median and from most of your other data. It's those values that schew your <code>mean</code>, pulling it to one or the other side of your <code>median</code>.</p>"},{"location":"datascience/statistics/concepts/#removing-outliers","title":"Removing outliers","text":"<p>To remove outliers, we do the following:</p> <ol> <li> <p>calculate the <code>medium</code>. Let's say for example that is <code>10</code> for our imaginary data sample.</p> </li> <li> <p>take the value that's 25% to the left (<code>Q1</code>) and right of the medium (<code>Q3</code>). Let's say <code>Q1 = 7</code> and <code>Q3 = 14</code></p> </li> <li> <p>calculate the <code>IQR</code> (inter quartile ranges). <code>IQR = Q3 - Q1</code>. In our imaginary scenario <code>IQR = 14 - 7 = 7</code>.</p> </li> <li> <p>multiply <code>IQR</code> by <code>1.5</code>. For our case, we get <code>IQR * 1.5 = 7 * 1.5 = 10.5</code></p> </li> <li> <p>we consider <code>outliers</code> all values that are outside the range [<code>Q1 - IQR*1.5</code>, <code>Q3 + IQR*1.5</code>]. So in our case, anything that's smaller than <code>Q1 - IQR*1.5 = 7 - 10.5 = -3.5</code>, and anything bigger than <code>Q3 + IQR*1.5 = 14 + 10.5 = 24.5</code>, will be removed from the data set.</p> </li> </ol> <p>An image representation of the above steps:</p> <p></p>"},{"location":"datascience/statistics/distributions/","title":"Distributions","text":"<p>Types of distributions</p>"},{"location":"datascience/statistics/distributions/#bernoulli","title":"Bernoulli","text":"<p>It's either successful or not. </p> <p>E.g. has a specific adult ever posted a video online?</p> <p>x = 1 (success/yes), with prob p</p> <p>x = 0 (fail/no), with prob 1-p</p> <p>A Bernoulli distribution is a special case of Binomial Distribution with a single trial.</p>"},{"location":"datascience/statistics/distributions/#binomial","title":"Binomial","text":"<p>Counts the total bernoulli variables values from a batch of data. E.g. how many drugs have worked on customers from a batch?</p> <p>Say we have 25 adults. We ask them if they posted a video or not. We can count how many have ever posted a video.</p> <p>Say <code>X</code> is the number of adults who ever posted a video. X coresponds to the binomial distribution.</p> <p>\\(\\(P(X = x) = p^x(1-p)^{n-x}\\)\\) Assumptions for binomial, for the above formula to work: - number of trials (n) is fixed - each trial is independent of the other trials - the are only two possible outcomes for each trial the probability of the success (p) is the same for each trial</p> <p>If we want to calculate the probability of <code>X</code> to be a specific value, say 10, we have: \\(P(X = 10) = p^{10}(1-p)^{25-10}\\)</p> <p>If let's say the probability of success is 0.3, then $P(X = 10) = 0.3^{10}0.7^{15} = 0.0000059049 * 0.004747561509943 = 2.8033876e-8 $. Pretty low probability.</p>"},{"location":"datascience/statistics/distributions/#uniform","title":"Uniform","text":"<p>All probabilities are the same. It's equal probable for the variable to take any value from the range of possible values. E.g. the probability of a number to be picked for a lotery game. It's theoretically the same. Or of a number to come on a dice etc.</p>"},{"location":"datascience/statistics/distributions/#descrete-uniform","title":"Descrete uniform","text":"<p>Limited number of options/results.</p> <p>e.g. month of the year (can be one of 12)</p>"},{"location":"datascience/statistics/distributions/#continuous-uniform","title":"Continuous uniform","text":"<p>A range of unlimited possible options/results.</p> <p>e.g. tomorrow's temperature (can be any number value, some having higher probability)</p> <p>Here's a comparison between discrete and continuous variables: | Discrete Variable                                                                            | Continuous Variable           | | --------------------------------------------------------------------| -------------------------------------------------------------------------------------------- | | Takes particular countable values (i.e. integers)                  | Takes any measured value within a given range (i.e. floats). | | Discrete data is information that has noticeable gaps between values.                                     | Continuous data is information that occurs in a continuous series.    | | Discrete data is made up of discrete or distinct values.                                  | Directly in opposition, continuous data includes any value that falls inside a range. | | Discrete data can be counted.                                          | Continuous data is quantifiable.| | Bar graphs are a visual representation of discrete data.   | Continuous data are graphically represented using a histogram.| | For discrete data, a classification like 10-19, 20-29,\u2026, etc., are non-overlapping or mutually inclusive. | For continuous data, classifications such as 10-20, 20-30, etc., overlap or are mutually exclusive.           | | The discrete function graph exhibits a distinct point that is nonetheless disconnected.                   | A broken line connects the points on a continuous function graph.                                             | | Examples of frequent discrete data include the number of students, children, shoe size, and so forth.     | Some common continuous data types are height, weight, time, temperature, age, etc.                            |</p>"},{"location":"datascience/statistics/distributions/#normal","title":"Normal","text":"<p>Logaritmic scale (Bell curve). E.g. income distribution of a developed country is generally so. Few rich, few poor, many middle.</p> <p>Takes two parameters: <code>mean</code> (miu) and the <code>standard deviation</code> (sigma). The values are distributed around the <code>mean</code> equally on both sides</p> <p>Caracteristics:</p> <ol> <li><code>mean</code>, <code>median</code> and <code>mode</code> of the normal distribution are equal</li> <li>the normal curve is symetric around the <code>mean</code></li> <li>the total area under the normal curve is 1</li> <li>100% of the data is under the curve</li> </ol>"},{"location":"datascience/statistics/distributions/#empirical-rule-of-normal-distribution","title":"Empirical Rule of normal distribution","text":"<p>The <code>Empirical Rule</code> says that about 68% of the population is within one standard deviation from the <code>mean</code>, about 95% is within 2 standard deviations from the <code>mean</code> and about 99% within 3 standard deviations.</p> <p></p>"},{"location":"datascience/statistics/distributions/#standard-normal-distribution","title":"Standard normal distribution","text":"<p>The standard distribution always has a mean of 0 and standard deviation of 1.</p> <p>To work with normal distributions, we first standardise them. See image below: </p> <p>We can convert a normal variable <code>X</code> to a standard normal variable <code>Z</code>, given <code>mu</code> is the mean and <code>sigma</code> the standard deviation, using: \\(\\(Z =  (X-mu)/sigma\\)\\) Z(Z-Score/Standard score) is the measure of the number of standard deviations above or below the mean (i.e. left and right of it) that data point falls within.</p> <p>E.g. if my height has Z-score 1. That means that my height is one standard deviation above average.</p> <pre><code>import numpy as np\nimport scipy.stats as stats\n\nfrom scipy.stats import norm\n\n# probability of data to be less than `x`\nnorm.cdf(x, mu, sigma)\n</code></pre> <pre><code>mu = 500 # mean of original popluation is known\nsigma = 25\nn = 100 # sample size\nalpha = 0.95 # error margin allowed is 0.05\nconfidence_interv = norm.interval(alpha, loc = mu, scale = sigma/np.sqrt(n))\n</code></pre>"},{"location":"datascience/statistics/distributions/#t-distribution","title":"t-distribution","text":"<p>We use the t-distribution when we don't know the standard deviation of the population.</p> <p>In this case, we estimate the standard distribution and calculate the confidence interval using the t-distribution.</p> <p>T-distribution uses a new term, compared with the normal distribution:</p> <p><code>degree of freedom</code> of the estimate = number of independent pieces of information that went into calculating the estimate (of the mean). It is always the number of samples used for the Central Limit Theorem, minus one (<code>n-1</code>).</p> <pre><code>mu = 500 # NOTE this is the calculated one based on random samples. It's the mean of the standard distribution of the means of samples - based on Central Limit Theorem\nsigma = 25\nn = 100 # sample size\nalpha = 0.95 # error margin allowed is 0.05\nconfidence_interv = t.interval(alpha, df = n-1, loc = mu, scale = sigma/np.sqrt(n))\n</code></pre>"},{"location":"datascience/statistics/hypothesis_testing/","title":"Hypothesis Testing","text":""},{"location":"datascience/statistics/hypothesis_testing/#hyphothesis","title":"Hyphothesis","text":"<p>Say we have a software that we know to be crashing about every 5 days. We have implmented a new version of the software and think that the version now only crashes less than every 30 days. </p> <p>H0 = status quo, nothing changed. E.g. the new software still crashes every 5 days</p> <p>H1 = assumption that something changed. E.g. the new software crashes every 30 days or more</p>"},{"location":"datascience/statistics/hypothesis_testing/#errors","title":"Errors","text":"H0 is true H0 is false Reject H0 Type 1 Error  ( prob = alpha ) Level of significance Correct  ( prob = 1 - beta )  Power of the test Fail to reject H0 Correct ( prob = 1 - alpha ) Type 2 Error  ( prob = beta )"},{"location":"datascience/statistics/hypothesis_testing/#type-i1-error","title":"Type I(1) error","text":"<p>When the hypothesis is correct but we reject it. We assume the status quo is still valid when it isn't.</p> <p>The probability to get a type 1 error is called the <code>level of significance</code> and is noted as <code>alpha</code>.</p> <p><code>alpha</code> is to be decided before the test and represents the error margin we consider acceptable, generally for business applications it's <code>5%</code>.</p> <p><code>alpha</code> depends on the business requirements, NOT the data.</p>"},{"location":"datascience/statistics/hypothesis_testing/#type-ii2-error","title":"Type II(2) Error","text":"<p>When the hypothesis is incorrect (status quo is still valid), but we don't reject it (we assume it had changed).</p>"},{"location":"datascience/statistics/hypothesis_testing/#p-value","title":"P-value","text":"<p>The <code>p-value</code> is the probability of observing evidence in favor of H0. If <code>p-value</code> is small, then we reject H0.</p> <p>Or put differently <code>The p-value is a measure of the probability that an observed difference could have occurred just by random chance.</code></p> <p>When <code>p-value</code> &gt;= <code>alpha</code>, then we reject the null hypothesis. Otherwise we don't.</p> <p>In other words, when the observed probability that H0 (status quo) is valid is bigger or equal to the error margin of the population, then we consider the status quo still valid.</p> <p>Or when the observed probability that H0 is valid is smaller than the accepted error margin, then there is a chance that H0 is invalid, thus we reject H0(status quo).</p>"},{"location":"datascience/statistics/hypothesis_testing/#steps","title":"Steps","text":""},{"location":"datascience/statistics/inferential/","title":"Inferential statistics","text":"<p>Inferential Statistics helps us to draw conclusions about a population by analyzing a representative sample from the population.</p>"},{"location":"datascience/statistics/inferential/#random-variable","title":"Random variable","text":"<p>Assigns a numerical value to each outcome of an experiement. It assumes different values with different probability.</p>"},{"location":"datascience/statistics/inferential/#discrete-random-variable","title":"Discrete random variable","text":"<p>Can have a limited number of values. E.g. <code>x</code> is a variable that can be: - 0, with prob 0.95 - 1, with prob 0.04 - 2, with prob 0.008 - 3, with prob 0.002</p> <p>We are able to list all the possible values - thus discrete.</p> <p>Note that the probabilities together should sum up to 1 (i.e. 100%)</p>"},{"location":"datascience/statistics/inferential/#continuous-random-variable","title":"Continuous random variable","text":"<p>We can't specify the value. E.g. a variable that's any <code>float</code> number in between 0 and 1. There are in infinite values in between, depending on what precision you're going to. You can't list all those values.</p>"},{"location":"datascience/statistics/inferential/#probablity-distribution","title":"Probablity distribution","text":"<p>Discribes the values that a random variable can take.</p>"},{"location":"datascience/statistics/inferential/#discrete-probability-distribution","title":"Discrete probability distribution","text":"<p>Probability for a discrete random variable value. It is described by a <code>probability mass function</code>, which gives a value to the chance a random variable taking a particular value out of the distribution.</p>"},{"location":"datascience/statistics/inferential/#continuous-probability-distribution","title":"Continuous probability distribution","text":"<p>Described by a <code>probability density function</code>, which helps determine the probability with which a randome variable lies between two given numbers.</p>"},{"location":"datascience/statistics/probability/","title":"Probability","text":""},{"location":"datascience/statistics/probability/#cumulative-probability","title":"Cumulative probability","text":"<p>The probability of several scenarios to take place. E.g. What are the chances that at least 8 people have uploaded a video online, out of 100 people, over a range of 20 experiments/trials. See binomial.</p> <p>```python import numpy as np import scipy.stats as stats</p>"},{"location":"datascience/statistics/probability/#estimate-probability-of-distribution-of-people-who-posted-a-video-online","title":"estimate probability of distribution of people who posted a video online.","text":"<p>n = 15 # number of trials p = 0.8 # probability of one person to have posted a video online ever k = np.arrange (0, 11) # 10 people</p>"},{"location":"datascience/statistics/probability/#calculate-the-pmf-probability-mass-function-use-binom-package-from-scipy-stats","title":"calculate the pmf - probability mass function. Use binom package from scipy stats","text":"<p>from scipy.stats import binom</p>"},{"location":"datascience/statistics/probability/#this-returns-an-array-with-the-probability-for-each-value-in-the-k-array-eg-the-first-value-will-be-the-probability-of-only-one-person-to-have-posted-a-video-after-the-15-trials-last-will-be-the-probability-of-10-people-all-the-people-to-have-posted-a-video-after-15-trials","title":"this returns an array with the probability for each value in the k array. E.g. the first value will be the probability of only one person to have posted a video, after the 15 trials. Last will be the probability of 10 people (all the people) to have posted a video after 15 trials.","text":"<p>binomial = binom.pmf(k=k, n=n, p=p) </p>"},{"location":"datascience/statistics/probability/#cumulative-probability-what-is-the-changeprobability-that-at-least-7-people-have-ever-posted-a-video-online","title":"cumulative probability. What is the change/probability that at least 7 people have ever posted a video online","text":"<p>binom.cdf(k=7, n=n, p=p) # note in this case k is the max (inclusive) number of people.  ```</p>"},{"location":"kubernetes/scheduler/","title":"Scheduler","text":"<p>Handles selecting a node for a workload/pod.</p>"},{"location":"kubernetes/scheduler/#filtering","title":"Filtering","text":"<p>Filters out nodes that can't host the pod. Can be due to: - not having enough resources - node might have a taint or affinity, restricting it to specific nodes - an organiation policy might set specific rules</p>"},{"location":"kubernetes/scheduler/#scoring","title":"Scoring","text":"<p>Assigns a score to nodes.</p>"},{"location":"kubernetes/scheduler/#scheduling-policies","title":"Scheduling policies","text":"<p>Allow creating <code>Predicates</code> for filtering and <code>Priorities</code> for scoring.</p>"},{"location":"kubernetes/scheduler/#scheduling-profiles","title":"Scheduling Profiles","text":"<p>Allows to configure Plugins that implement different scheduling stages, including: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code> and others. You can also configure the kube-scheduler to run different profiles.</p> <p></p>"},{"location":"opentelemetry/concepts/","title":"Concepts","text":""},{"location":"opentelemetry/concepts/#signals","title":"Signals","text":"<p>system outputs that describe the underlining</p>"},{"location":"opentelemetry/traces/","title":"Traces","text":""},{"location":"opentelemetry/traces/#trace","title":"Trace","text":"<p>Gives us the big picture of what happens when a request is made to an application</p>"},{"location":"opentelemetry/traces/#span","title":"Span","text":"<p>A span is a unit of work, or operation. It is usually a function call or a part of a function, whatever is relevant.</p> <p>A span is part of a trace. </p> <p>Spans can be <code>root spans</code> (no parent id) or child spans. You can wrap one in another as you see fit.</p>"}]}